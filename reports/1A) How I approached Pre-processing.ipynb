{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building my dataset\n",
    "\n",
    "I decided to work with just one dataset for now, for trial purposes. I will add on to this dataset after testing architectures. I opted for the **\"NSFW Data Source URLs\" developed by Data Scientist Evgeny Bazarov.** This is a large and high-quality image dataset of sexually explicit images containing over 1.58 million data volumes in 159 categories. \n",
    "\n",
    "Link: https://github.com/EBazarov/nsfw_data_source_urls\n",
    "\n",
    "I examined the contents of the folders and devised a method of approach for data extraction as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Step 1: Determining the type of classifier and extracting URL lists\n",
    "\n",
    "I have decided to construct a binary classifier (two outcomes: NSFW or not-NSFW). I noticed that for each category of NSFW, it had multiple sub-categories. We do not need such a level of intricacy. I noticed that inside the files of each category is a .txt file that contains all the URLs of the subcategories. So, I extracted those specific .txt files and merged them into one COMPLETE.txt file - which contains all URLs (1,586,929)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import os, shutil, glob\n",
    "import requests\n",
    "\n",
    "\"\"\"Extracting all .txt files to store to one directory and then combining all .txt files\"\"\"\n",
    "\n",
    "for extract_directory in os.listdir(r'C:\\\\Users\\\\Harshinee\\\\Desktop\\\\Outreachy\\\\Wikimedia\\\\NSFW Classifier\\\\Data\\\\raw_data'):\n",
    "    file_name = \"C:\\\\Users\\\\Harshinee\\\\Desktop\\\\Outreachy\\\\Wikimedia\\\\NSFW Classifier\\\\Data\\\\raw_data\\\\\"+extract_directory\n",
    "    os.chdir(file_name)\n",
    "    for file in glob.glob(\"*.txt\"):\n",
    "        shutil.move(file, \"C:\\\\Users\\\\Harshinee\\\\Desktop\\\\Outreachy\\\\Wikimedia\\\\NSFW Classifier\\\\Data\\\\binary_classifier\\\\\")\n",
    "        \n",
    "filenames = []\n",
    "for file in os.listdir(r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier'):\n",
    "    filenames.append(str(file))\n",
    "\n",
    "os.chdir(r'C:\\\\Users\\\\Harshinee\\\\Desktop\\\\Outreachy\\\\Wikimedia\\\\NSFW Classifier\\\\Data\\\\binary_classifier')\n",
    "os.getcwd()\n",
    "\n",
    "with open(r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\COMPLETE.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf8\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Step 2: Downloading photos from COMPLETE.txt and examining them\n",
    "\n",
    "Next, I downloaded the photos via scanning COMPLETE.txt. For now, I let this script run for 3 hours and extracted xxx images.  After the script was finally completely executed, I examined the dataset. I removed the obvious false positives (random images that aren't NSFW) to create a pure dataset containing NSFW images. I think this should be enough because I will be using transfer learning, instead of training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Harshinee\\\\Desktop\\\\Outreachy\\\\Wikimedia\\\\NSFW Classifier\\\\Data\\\\binary_classifier')\n",
    "os.getcwd()\n",
    "\n",
    "file1 = open('COMPLETE.TXT', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "img_index = 0\n",
    "\n",
    "for line in Lines:\n",
    "    try:\n",
    "        response = requests.get(line)\n",
    "        file = open(\"image \" + str(img_index) + \".jpg\", \"wb\")\n",
    "        img_index = img_index+1\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Step 3: Developing an SFW dataset\n",
    "\n",
    "Initially I thought that I could just build a classifier with 1 class, because non-NSFW content could be anything. Unfortunately, upon reading about this method of approach, I realized that it could make the classifier less competent because it would have to rely on the exact features developed by the training set of NSFW content, instead of understanding why it's NSFW. Like rote learning of sorts. \n",
    "\n",
    "So, for now, I have decided to create an SFW class containing random images extracted from Lorem Picsum (https://picsum.photos/). I made sure that the dimensions of each image were randomly selected from 2 sets (dimension_1 and dimension_2). I downloaded 2000 SFW pictures as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\SFW')\n",
    "\n",
    "dimension_1 = [500, 801, 750, 2448, 1512, 332, 908, 3008, 800, 1200]\n",
    "dimension_2 = [332, 908, 3008, 800, 1200, 500, 801, 750, 2448, 1512]\n",
    "\n",
    "for i in range(2000):\n",
    "    SFW_image_generator = r'https://picsum.photos/' + str(random.choice(dimension_1)) + r'/' + str(random.choice(dimension_2))\n",
    "    response = requests.get(SFW_image_generator)\n",
    "    file = open(\"image \" + str(i) + \".jpg\", \"wb\")\n",
    "    i = i+1\n",
    "    file.write(response.content)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Step 4: Removing duplicates\n",
    "\n",
    "The following script helps remove images that are the same but have different file names.\n",
    "\n",
    "**[Caution]: The original .ipynb I used for work on contains samples of these duplicates and includes NSFW images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\NSFW'\n",
    "os.chdir(filepath)\n",
    "def file_hash(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return md5(f.read()).hexdigest()\n",
    "\n",
    "duplicates = []\n",
    "hash_keys = dict()\n",
    "for index, filename in  enumerate(os.listdir('.')):  #listdir('.') = current directory\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            filehash = hashlib.md5(f.read()).hexdigest()\n",
    "        if filehash not in hash_keys: \n",
    "            hash_keys[filehash] = index\n",
    "        else:\n",
    "            duplicates.append((index,hash_keys[filehash]))\n",
    "            \n",
    "print(duplicates)\n",
    "\n",
    "file_list = os.listdir()\n",
    "for file_indexes in duplicates[:30]:\n",
    "    try:\n",
    "    \n",
    "        plt.subplot(121),plt.imshow(imread(file_list[file_indexes[1]]))\n",
    "        plt.title(file_indexes[1]), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "        plt.subplot(122),plt.imshow(imread(file_list[file_indexes[0]]))\n",
    "        plt.title(str(file_indexes[0]) + ' duplicate'), plt.xticks([]), plt.yticks([])\n",
    "        plt.show()\n",
    "    \n",
    "    except OSError as e:\n",
    "        continue\n",
    "        \n",
    "for index in duplicates:\n",
    "    os.remove(file_list[index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\SFW'\n",
    "os.chdir(filepath)\n",
    "def file_hash(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return md5(f.read()).hexdigest()\n",
    "\n",
    "duplicates = []\n",
    "hash_keys = dict()\n",
    "for index, filename in  enumerate(os.listdir('.')):  #listdir('.') = current directory\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            filehash = hashlib.md5(f.read()).hexdigest()\n",
    "        if filehash not in hash_keys: \n",
    "            hash_keys[filehash] = index\n",
    "        else:\n",
    "            duplicates.append((index,hash_keys[filehash]))\n",
    "            \n",
    "print(duplicates)\n",
    "\n",
    "file_list = os.listdir()\n",
    "for file_indexes in duplicates[:30]:\n",
    "    try:\n",
    "    \n",
    "        plt.subplot(121),plt.imshow(imread(file_list[file_indexes[1]]))\n",
    "        plt.title(file_indexes[1]), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "        plt.subplot(122),plt.imshow(imread(file_list[file_indexes[0]]))\n",
    "        plt.title(str(file_indexes[0]) + ' duplicate'), plt.xticks([]), plt.yticks([])\n",
    "        plt.show()\n",
    "    \n",
    "    except OSError as e:\n",
    "        continue\n",
    "        \n",
    "for index in duplicates:\n",
    "    os.remove(file_list[index[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Step 5: Splitting the data into train, validation, and test sets\n",
    "\n",
    "The following simple script splits the image datasets into train (60% - 1200 images), validation (20% - 400 images) and test (20% - images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\NSFW')\n",
    "\n",
    "for i in range(1200):\n",
    "    file = random.choice(os.listdir())\n",
    "    shutil.move(file, r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\Train\\NSFW')\n",
    "\n",
    "for i in range(400):\n",
    "    file = random.choice(os.listdir())\n",
    "    shutil.move(file, r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\Test\\NSFW')\n",
    "\n",
    "for i in range(400):\n",
    "    file = random.choice(os.listdir())\n",
    "    shutil.move(file, r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\Validation\\NSFW')\n",
    "\n",
    "os.chdir(r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\SFW')\n",
    "\n",
    "for i in range(1200):\n",
    "    file = random.choice(os.listdir())\n",
    "    shutil.move(file, r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\Train\\SFW')\n",
    "\n",
    "for i in range(400):\n",
    "    file = random.choice(os.listdir())\n",
    "    shutil.move(file, r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\Validation\\SFW')\n",
    "\n",
    "for i in range(400):\n",
    "    file = random.choice(os.listdir())\n",
    "    shutil.move(file, r'C:\\Users\\Harshinee\\Desktop\\Outreachy\\Wikimedia\\NSFW Classifier\\Data\\binary_classifier\\Test\\SFW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
